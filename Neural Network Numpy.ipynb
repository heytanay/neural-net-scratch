{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return max(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return (1/(1+np.exp(-z)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \"\"\"\n",
        "    \n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 4\n",
        "    n_y = Y.shape[0] # size of output layer\n",
        "    \n",
        "    return (n_x, n_h, n_y)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Function:\n",
        "    Generates Weights (random) and Biases (zeros) for the 2 layer neural network\n",
        "    \n",
        "    \"\"\"\n",
        "        \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    \n",
        "    paramters = {\n",
        "        'W1': W1,\n",
        "        'b1': b1,\n",
        "        'W2': W2,\n",
        "        'b2': b2,\n",
        "    }\n",
        "    \n",
        "    return paramters"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "  \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "  \n",
        "  \"\"\"\n",
        "  # Extract the Weights and Bias from parameters dictionary\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  \n",
        "  # Calculate multiple steps of forward propagation and at the end calculate A2 Probabilities\n",
        "  Z1 = np.matmul(W1, X) + b1\n",
        "  A1 = np.tanh(Z1)\n",
        "  Z2 = np.matmul(W2, A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "  \n",
        "  cache = {\n",
        "    'Z1': Z1,\n",
        "    'A1': A1,\n",
        "    'Z2': Z2,\n",
        "    'A2': A2\n",
        "  }\n",
        "  \n",
        "  return cache"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_cost(A2, Y, parameters):\n",
        "  \"\"\"\n",
        "    Computes the cross-entropy cost\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "  \n",
        "  \"\"\"\n",
        "  # Get the total number of examples\n",
        "  m = Y.shape[1]\n",
        "  \n",
        "  # Compute the Cross-entropy cost\n",
        "  logprob = (np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), (1-Y)))\n",
        "  \n",
        "  # Squeeze the Numpy array (removes the extra dimensions)\n",
        "  cost = np.squeeze(-(1/m) * np.sum(logprob))\n",
        "  \n",
        "  return cost"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(parameters, cache, X, Y):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "  \"\"\"\n",
        "  \n",
        "  m = X.shape[1]\n",
        "  \n",
        "  # Get the Weights from 'paramters' dictionary\n",
        "  W1 = parameters['W1']\n",
        "  W2 = parameters['W2']\n",
        "  \n",
        "  # Retrieve the respective activations from 'cache' dictionary\n",
        "  A1 = cache['A1']\n",
        "  A2 = cache['A2']\n",
        "  \n",
        "  # Calculate respective derivatives\n",
        "  dZ2 = A2 - Y # Derivative of Final layer output is final Activation - Target value (Predicted - Original)\n",
        "  dW2 = (1/m) * np.matmul(dZ2, A1.T) # Derivative of Second layer weights is multiplication of dZ2 and A1.T, averaged over all samples\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "  dZ1 = np.matmul(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
        "  dW1 = (1/m) * np.matmul(dZ1, X.T)\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "  \n",
        "  gradients = {\n",
        "    'dW1': dW1,\n",
        "    'db1': db1,\n",
        "    'dW2': dW2,\n",
        "    'db2': db2,\n",
        "  }"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_paramters(parameters, gradients, learning_rate = 1):\n",
        "  \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  # Get the Paramters from the Dictionary\n",
        "  W1 = paramters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  \n",
        "  # Get the gradients from the dictionary\n",
        "  dW1 = gradients['dW1']\n",
        "  db1 = gradients['db1']\n",
        "  dW2 = gradients['dW2']\n",
        "  db2 = gradients['db2']\n",
        "  \n",
        "  # Run Gradient Descent for all weights and biases\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "  \n",
        "  # Pack the parameters into a dictionary\n",
        "  paramters = {\n",
        "    'W1': W1,\n",
        "    'b1': b1,\n",
        "    'W2': W2,\n",
        "    'b2': b2,\n",
        "  }\n",
        "  \n",
        "  return paramters"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(X, Y, n_h, epochs=10000, print_cost = False):\n",
        "  \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  # Make new n_x and n_y\n",
        "  n_x = layer_sizes(X, Y)[0]\n",
        "  n_y = layer_sizes(X, Y)[2]\n",
        "  \n",
        "  # Initialize paramters\n",
        "  parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  \n",
        "  \n",
        "  # Run the loop for training\n",
        "  for epoch in range(epochs):\n",
        "    cache = forward_propagation(X, parameters=parameters)\n",
        "    \n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    cost = calc_cost(A2, Y, parameters)\n",
        "    \n",
        "    grads = backward_prop(parameters, cache, X, Y)\n",
        "    \n",
        "    parameters = update_paramters(parameters, grads)\n",
        "    \n",
        "    if print_cost and epoch % 1000 == 0:\n",
        "      print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        "      \n",
        "  return parameters"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(parameters, X):\n",
        "  \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "  \"\"\"\n",
        "  \n",
        "  cache = forward_propagation(X, parameters)\n",
        "  A2 = cache['A2']\n",
        "  \n",
        "  predictions = (A2 > 0.5)\n",
        "  \n",
        "  return predictions"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "version": "3.5.6",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "latex_envs": {
      "report_style_numbering": false,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "LaTeX_envs_menu_present": true,
      "autocomplete": true,
      "autoclose": false,
      "eqNumInitial": 1,
      "bibliofile": "biblio.bib",
      "latex_user_defs": false,
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "labels_anchors": false,
      "cite_by": "apalike",
      "user_envs_cfg": false
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.14.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}